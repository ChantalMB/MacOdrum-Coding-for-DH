{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY SOLUTIONS\n",
    "This python notebook evaluates and compares the solutions for the workshop example. There are three solutions in this notebook with direct references to code we've seen in Python notebooks I and II.\n",
    "\n",
    "Each solution shares the same \"Setup\", which will be highlighted below, and even the setup will have its first mentions tagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n",
    "All solutions will have this setup code required, especially if the solution is being presented in a different cell or notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install pandas\n",
    "# Installing libraries is not a requirment for the solution to funciton, but it's good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 1-2\n",
    "#The practice of importing libraries and collections are mentioned there. Specific items such as NLTK's collections and pandas are mentioned in Notebook II across multiple cells. \n",
    "\n",
    "nltk.download('stopwords')\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 11-25\n",
    "#Stopwords are further streamlined in Notebook II, code block 12 (NLTK) as a shortcut to that first one.\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "#First Appearance: Notebook II, block 2 (Pandas) lines 3-4.\n",
    "#Completely optional, limits the amount of rows viewed whenever a csv is being printed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that variables are declared throughout all code in both notebooks, they can be ...variable to user preference.\n",
    "\n",
    "litrev_df = pd.read_csv('digihum-lit-rev.csv', delimiter=\",\")\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 28\n",
    "#This is actually the pandas application of the same fucntion, first seen in Notebook II, code block 2 (Pandas) line 7\n",
    "\n",
    "litrev_df\n",
    "#First Appearance: Notebook II, block 2 (Pandas) line 8\n",
    "#Declaring a (done in the line above) is mentioned on multiple occasions, it is basically a pratice or indicator that a variable or bucket has the file attached and readable for further analysis. \n",
    "#Though optional, it's perfect for learning what the column names are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the minimum requirements for all solutions, or the \"Setup\" for them. \n",
    "\n",
    "# SOLUTION A\n",
    "\n",
    "This solution is a \"Brute force\" method, where the .csv file is forced out as a .txt file and there isn't much preprocessing involved. You will get from Point A to B without inlcusion of stopwords or punctuation checking. This is a very NLTK oriented approach. Though this isn't a clean \"solution\", it can help you figure out your next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "#First appearance: Notebook II, code block 3 (NLTK), required for tokenization. \n",
    "#PUNKT is a resouce part of the nltk library that contains many punctuation tools. It's very useful to not include punctuation in various analyses or text frequencies.\n",
    "\n",
    "from nltk import FreqDist\n",
    "#First appearance: Notebook II, code block 7 (NLTK), required to run a frequency distribution. \n",
    "#FreqDist is a set of functions and tools that streamlines the act of operating on a Frequency Distribution\n",
    "\n",
    "description_list = list(litrev_df[\"Description\"])\n",
    "#First Appearance: Notebook II, block 4 (Pandas), creates a new variable that looks at the dataframe items as a list, easier for export.\n",
    "\n",
    "litrev_df = litrev_df.loc[litrev_df[\"Description\"].notna()]\n",
    "#First Appearance: Notebook II, block 7 (Pandas), only examines that don't have NA decription. Useful for strings.\n",
    "\n",
    "litrev_df[\"Description\"] = litrev_df[\"Description\"].str.lower()\n",
    "#First Appearance: Notebook II, block 13 (Pandas), changes all text to lowercase strings to help NLTK work with it. \n",
    "\n",
    "litrev_df.to_csv(\"test.txt\")\n",
    "#First Appearance: Notebook II, block 15 (Pandas), forces a file to export as a .txt file for easier NLTK reading\n",
    "\n",
    "description = open('test.txt', encoding=\"utf-8\").read().lower()\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 28, text exclusive\n",
    "\n",
    "description_words = word_tokenize(description)\n",
    "#First Appearance: Notebook II, block 5 (NLTK), demonstartion of tokenizing words\n",
    "\n",
    "descriptionfrequency = FreqDist(description_words)\n",
    "descriptionfrequency.most_common(10)\n",
    "#First Appearance: Notebook II, block 7 (NLTK), using the frequency distribution functions in NLTK to get a pretty, mediocre result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION B\n",
    "Here are applications of the more primative loops for actual results. Can be streamlined further though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 1-2, the only mention and usage of the regex library and the collections library with the coutner function. This is later replaced by the Frequency Distribution functions in NLTK\n",
    "\n",
    "description_list = list(litrev_df[\"Description\"])\n",
    "#First Appearance: Notebook II, block 4 (Pandas), creates a new variable that looks at the dataframe items as a list, easier for export.\n",
    "\n",
    "litrev_df = litrev_df.loc[litrev_df[\"Description\"].notna()]\n",
    "#First Appearance: Notebook II, block 7 (Pandas), only examines that don't have NA decription. Useful for strings.\n",
    "\n",
    "litrev_df[\"Description\"] = litrev_df[\"Description\"].str.lower()\n",
    "#First Appearance: Notebook II, block 13 (Pandas), changes all text to lowercase strings to help NLTK work with it. \n",
    "\n",
    "litrev_df.to_csv(\"test.txt\")\n",
    "#First Appearance: Notebook II, block 15 (Pandas), forces a file to export as a .txt file for easier NLTK reading\n",
    "\n",
    "description = open('test.txt', encoding=\"utf-8\").read().lower()\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 28, text exclusive\n",
    "\n",
    "\n",
    "word_count = 50\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 8. This is a variable cap for the amount of words that can be viewed in the output\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "             \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n",
    "             'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "             'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', \n",
    "             'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'said', \n",
    "             'say', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
    "             'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', \n",
    "             'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "             'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'would', 'could', 'should', \n",
    "             'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', \n",
    "             'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \n",
    "             \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", \n",
    "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \n",
    "             \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \n",
    "             \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 11-25. This is a brute stopwords list to be used instead of the NLTK dictionary. \n",
    "\n",
    "text_split_into_words = re.split(r\"\\W+\", description)\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 31. This uses the regex (Import re) library's split function to put the text file into words that are countable. \n",
    "\n",
    "significant_words = []\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 35. This is the first use of a an empty list, to be used in a loop.\n",
    "\n",
    "for word in text_split_into_words:\n",
    "    if word not in stop_words and word.isalpha():\n",
    "        significant_words.append(word)\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 37-39. This is the first loop that processes and updates the list, this also checks if the words are actually words. Thus creating a dictionary or a list of terms.\n",
    "\n",
    "significant_words_tally = Counter(significant_words)\n",
    "order_significant_words = significant_words_tally.most_common(word_count)\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) lines 42-45. This tallys and counts up the words, to display them by most common using tools found in the collections library. This is later replaced by NLTK's Frequency Distribution functions. \n",
    "\n",
    "order_significant_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION C\n",
    "\n",
    "There is more pre-processing involved which makes this more functional and the ideal middle ground of coding comprehension, problem solving, and creativity üëè!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#First appearance: Notebook II, code block 3 (NLTK), required for tokenization and stopwords library\n",
    "\n",
    "from nltk import FreqDist\n",
    "#First appearance: Notebook II, code block 7 (NLTK), required to run a frequency distribution. \n",
    "\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "#First Appearance: Notebook II, block 8 (NLTK), lines 4-5, adds a list of punctuation to ignore in the frequency distribution\n",
    "\n",
    "description_list = list(litrev_df[\"Description\"])\n",
    "#First Appearance: Notebook II, block 4 (Pandas), creates a new variable that looks at the dataframe items as a list, easier for export.\n",
    "\n",
    "litrev_df = litrev_df.loc[litrev_df[\"Description\"].notna()]\n",
    "#First Appearance: Notebook II, block 7 (Pandas), only examines that don't have NA decription. Useful for strings.\n",
    "\n",
    "litrev_df[\"Description\"] = litrev_df[\"Description\"].str.lower()\n",
    "#First Appearance: Notebook II, block 13 (Pandas), changes all text to lowercase strings to help NLTK work with it. \n",
    "\n",
    "litrev_df.to_csv(\"test.txt\")\n",
    "#First Appearance: Notebook II, block 15 (Pandas), forces a file to export as a .txt file for easier NLTK reading\n",
    "\n",
    "description = open('test.txt', encoding=\"utf-8\").read().lower()\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 28, text exclusive\n",
    "\n",
    "description_words = word_tokenize(description)\n",
    "#First Appearance: Notebook II, block 5 (NLTK), demonstartion of tokenizing words\n",
    "\n",
    "## HERE IS WHERE THE MAJOR DIFFERENCES ARE\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "#First Appearance: Notebook II, block 6 (NLTK), declaring the stopwords list as a variable for a loop and declaring using the english dictionary\n",
    "\n",
    "stop_words.extend([\"n't\", \"'s\", 'would',])\n",
    "#First Appearance: Notebook II, block 8 (NLTK), line 10, adds more words to the stopword list\n",
    "\n",
    "filtered_transcript_words = []\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 35. this creates and empty list to store all the processed words in\n",
    "\n",
    "for word in description_words:\n",
    "    if word not in stop_words and word not in punctuation:\n",
    "        filtered_transcript_words.append(word)\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 37-39, Perfected in Notebook II, block 9 (NLTK), lines 2-5, filters through words removing stopwords and punctuations to create a clean list\n",
    "\n",
    "descriptionfrequency = FreqDist(description_words)\n",
    "descriptionfrequency.most_common(10)\n",
    "#First Appearance: Notebook II, block 7 (NLTK), using the frequency distribution functions in NLTK to get a pretty, mediocre result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION D\n",
    "\n",
    "Chantal's original solution, a full display of using libraries to their maximum potential! This is above and beyond so we don't expect you to get here, but will be very impressed if you do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litrev_df = litrev_df[litrev_df[\"Description\"].notna()]\n",
    "#First Appearance: Notebook II, block 7 (Pandas), only examines that don't have NA decription. Useful for strings.\n",
    "\n",
    "abstracts_as_text = \"\"\n",
    "#First Appearance: Notebook I, block 4 (Naming Variables), examines how to declare an empty string, but there is no further mention of this as an application\n",
    "\n",
    "for i in litrev_df[\"Description\"]:\n",
    "    abstracts_as_text = abstracts_as_text + i + \"\\n\"\n",
    "#NO previous appearance - This loop is meant to look at each cell item as text, organizing the strings into new lines. This is a missing gap that most participants face if reaching for this solution.\n",
    "    \n",
    "abstractTokens = word_tokenize(abstracts_as_text.lower())\n",
    "#MULTIPLE APPEARANCES - word_tokenize uses code from Notebook II, block 5 (NLTK), abstracts_as_text is a variable and the .lower() function for the variable is used when reading files and converting cells to strings in  Notebook II, block 13 (Pandas).\n",
    "\n",
    "cleaned_abstractTokens = []\n",
    "#First Appearance: Notebook I, block 1 (A simple python script) line 35. This is the first use of a an empty list, to be used in a loop.\n",
    "\n",
    "for word in list(abstractTokens):\n",
    "    if word not in stopwords.words(\"english\") and word.isalpha():\n",
    "        cleaned_abstractTokens.append(word)\n",
    "#MULTIPLE APPEARANCES - This is a cleaned up combination of the loops found in Notebook I, block 1 (A simple python script) line 37-39 AND Notebook II, block 9 (NLTK), lines 2-5. The cobination is done using the stopwords.words('english') dictionary, and checking if a word.isalpha() instead of using punctuation.\n",
    "\n",
    "abstracts_df = pd.DataFrame(cleaned_abstractTokens, columns =['uniqueWords'])\n",
    "#NO previous appearance - This is a line of code that creates a new dataframe to house all the cleaned tokens from the previous loop, this places those tokens (words) ina  a column named \"uniqueWords\", this is part of that vital gap connecting using pandas dataframe cells in NLTK.\n",
    "        \n",
    "keywords = abstracts_df[\"uniqueWords\"].value_counts()\n",
    "#First Appearance: Notebook II, block 9 (Pandas), this counts the values using a pandas function, meaning that this version never uses LTK besides the tokenizations and stopwords libraries. \n",
    "\n",
    "keywords\n",
    "#Declaring the variable for printing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macodrum-coding-dh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
