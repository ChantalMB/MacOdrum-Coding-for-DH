{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cadfbd47",
   "metadata": {},
   "source": [
    "# Loops\n",
    "\n",
    "Loops are used to repeat a process over and over until a given condition is met. It is similar to our process of, for example, searching for a specific quote in a text.\n",
    "\n",
    "Read sentence, \"Is this the sentence I'm looking for?\" you asking yourself, \"No\" your brain affirms-- and so you repeat this process of reading each sentence until you find the one you're looking for. Translating this process into computer, it would look something like this:\n",
    "\n",
    "```\n",
    "for every sentence on this page:\n",
    "    if this is the quote I am looking for:\n",
    "        I can stop reading, I've found it!\n",
    "    if this isn't the quote:\n",
    "        Let's read the next sentence.\n",
    "```\n",
    "\n",
    "In DH applications, for loops allow you to search a large amount of data very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d592078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a list of names\n",
    "list_of_names = [\"George Atherton\",\n",
    "                \"Marian Hyde\",\n",
    "                \"Sybil Dickenson\",\n",
    "                \"Sabina Dobson\",\n",
    "                \"Jessica Bradbury\",\n",
    "                \"Cindy Salter\",\n",
    "                \"Carolina McCabe\",\n",
    "                \"Glynis Graves\",\n",
    "                \"Laurie Dobson\",\n",
    "                \"Phoebe Watkins\",\n",
    "                \"Noel Boardman\"]\n",
    "\n",
    "# first, we can make a loop to \"iterate\" over the list with no conditions\n",
    "# it will simply continue to go over each item in the list until there is nothing left\n",
    "# to simply just print out every name in the list:\n",
    "\n",
    "for name in list_of_names:\n",
    "    print(name)\n",
    "\n",
    "# NOTE: \"name\" is a variable declared only in the loop, and it stores the item that the loop is presently looking at\n",
    "# in our case, in the first loop \"name\" = \"George Atherton\", and then after that name is printed, the loop repeats and \"name\" = \"Marian Hyde\", in the next loop \"name\" = \"Sybil Dickenson\", and so on until the end of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47484b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, like stated earlier, you'll more likely want to use a loop to find something relevant to your work\n",
    "# let's say we're only interested in people with the surname \"Dobson\"\n",
    "# we can use a combination of for loops and if statements to create a new list of only Dobsons!\n",
    "\n",
    "# declare your new list that we will add to\n",
    "only_dobsons = []\n",
    "\n",
    "for name in list_of_names:\n",
    "    # we check if this name includes \"Dobson\"\n",
    "    if \"Dobson\" in name:\n",
    "        # if this is True, we add this name to our new list\n",
    "        only_dobsons.append(name)\n",
    "\n",
    "print(only_dobsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937f2476",
   "metadata": {},
   "source": [
    "## Practice Activity #4: Loop and Look 👀\n",
    "Here is a list of quotes from the novel *Little Women* by Louisa May Alcott. In this activity, use a `for` loop and `if` statement as done above to find quotes that include the word **\"work\"**. These quotes should be added to a new list, then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "little_women_quotes = [\n",
    "                        \"...I do think washing dishes and keeping things tidy is the worst work in the world. It makes me cross; and my hands get so stiff, I can't practise well at all.\",\n",
    "                        \"I don't see how you can write and act such splendid things, Jo. You're a regular Shakespeare!\",\n",
    "                        \"But it does seem so nice to have little suppers and bouquets, and go to parties, and drive home, and read and rest, and not work. It's like other people, you know, and I always envy girls who do such things; I'm so fond of luxury\",\n",
    "                        \"She caught up her knitting, which had dropped out of her hands, gave me a sharp look through her specs, and said, in her short way, 'Finish the chapter, and don't be impertinent, miss.'\",\n",
    "                        \"You may try your experiment for a week, and see how you like it. I think by Saturday night you will find that all play and no work is as bad as all work and no play.\"\n",
    "                    ]\n",
    "\n",
    "for i in little_women_quotes:\n",
    "    if \"work\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec82c34a",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "'Functions' are blocks of reuseable code; as you know by now, in Python there are many functions such as `print()` or `len()` which were designed to perform specific tasks when called. If in your own code you believe that there is a task you will need to repeat multiple times at various points, you can write a function yourself! For example, instead of having something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12664f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find age of each person from records\n",
    "life_records = [[\"Cindy Salter\", \"Born: 1903\", \"Died: 1933\"], [\"Glynis Graves\", \"Born: 1911\", \"Died: 1989\"], [\"Noel Boardman\", \"Born: 1908\", \"Died: 1972\"]]\n",
    "\n",
    "cs_born = life_records[0][1]\n",
    "cs_death = life_records[0][2]\n",
    "\n",
    "# get only the number\n",
    "for word in cs_born.split():\n",
    "    if word.isdigit():\n",
    "        cs_born = int(word)\n",
    "\n",
    "for word in cs_death.split():\n",
    "    if word.isdigit():\n",
    "        cs_death = int(word)\n",
    "\n",
    "cs_age = cs_death - cs_born\n",
    "print(life_records[0][0] + \"'s age: \" + str(cs_age))\n",
    "\n",
    "gg_born = life_records[1][1]\n",
    "gg_death = life_records[1][2]\n",
    "\n",
    "# get only the number\n",
    "for word in gg_born.split():\n",
    "    if word.isdigit():\n",
    "        gg_born = int(word)\n",
    "\n",
    "for word in gg_death.split():\n",
    "    if word.isdigit():\n",
    "        gg_death = int(word)\n",
    "\n",
    "gg_age = gg_death - gg_born\n",
    "print(life_records[1][0] + \"'s age: \" + str(gg_age))\n",
    "\n",
    "nb_born = life_records[2][1]\n",
    "nb_death = life_records[2][2]\n",
    "\n",
    "# get only the number\n",
    "for word in nb_born.split():\n",
    "    if word.isdigit():\n",
    "        nb_born = int(word)\n",
    "\n",
    "for word in nb_death.split():\n",
    "    if word.isdigit():\n",
    "        nb_death = int(word)\n",
    "\n",
    "nb_age = nb_death - nb_born\n",
    "print(life_records[2][0] + \"'s age: \" + str(nb_age))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49a6d748",
   "metadata": {},
   "source": [
    "...We could have something much tidier and easier to read, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find age of each person from records\n",
    "life_records = [[\"Cindy Salter\", \"Born: 1903\", \"Died: 1933\"], [\"Glynis Graves\", \"Born: 1911\", \"Died: 1989\"], [\"Noel Boardman\", \"Born: 1908\", \"Died: 1972\"]]\n",
    "\n",
    "def find_age(record):\n",
    "    born = ''\n",
    "    death = ''\n",
    "    for word in record[1].split():\n",
    "        if word.isdigit():\n",
    "            born = int(word)\n",
    "\n",
    "    for word in record[2].split():\n",
    "        if word.isdigit():\n",
    "            death = int(word)\n",
    "\n",
    "    age = record[0] + \"'s age: \" + str(death - born)\n",
    "    return age\n",
    "\n",
    "print(find_age(life_records[0]))\n",
    "print(find_age(life_records[1]))\n",
    "print(find_age(life_records[2]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa67a40d",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "Now, what can make your code even *tidier*, plus easier to read *and* write? Libraries! Also referred to as \"packages\", these helpful tools are essentially large collections of pre-written functions that you can install in your Python environment and import so that you can use these functions in your code. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce45a0a0",
   "metadata": {},
   "source": [
    "## NLTK (Natural Language Tool Kit)\n",
    "\n",
    "In this workshop, we will be introducing two libraries which are necessities for any digital humanist's tool kit the first of which is [NLTK (Natural Language Tool Kit)](https://www.nltk.org/). This is an all-encompassing library to support work in natural language processing (NLP), a multidisciplinary field which deals with the interactions between \"natural\" human language and computers. It has its roots in linguisitics which is why it can do things like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbce7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# tagging PoS in inputted text\n",
    "text = word_tokenize(\"Be careful with that butter knife.\")\n",
    "nltk.pos_tag(text)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f97b55f",
   "metadata": {},
   "source": [
    "...But as the `word_tokenize()` function hints at, NLTK is also excellent at preparing text for and performing textual analysis in a less particulated manner!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1c14330",
   "metadata": {},
   "source": [
    "(**Note**: NLTK uses the Penn Treebank Tag Set for POS tagging, [which can be found here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ab267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# tokenization is the process of splitting strings into their individual \"tokens\"\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# to import a .txt file we use the \"open\" function, giving it the path to our text file and an instrution about what we want to do with the file\n",
    "# here, we would like to \"read\" our file into a variable so \n",
    "transcript = open('Bette-Smith-Transcript.txt').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could then tokenize by sentence, which splits the text into sentences\n",
    "transcript_sentences = sent_tokenize(transcript)\n",
    "transcript_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a14bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or more commonly, we can tokenize into words, which splits the sentences into its parts of speech\n",
    "transcript_words = word_tokenize(transcript)\n",
    "transcript_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now remember that huge block of stopwords manually typed out in the sample block of code from the first lesson? That comes built in to NLTK as you may have guessed from the earlier import statment\n",
    "# we can assign the NLTK stopwords to a variable like so:\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# and then remove the stopwords from out text using a loop to check if each word in the transcript and only keep the words that are NOT in out stopword list\n",
    "filtered_transcript_words = []\n",
    "for word in transcript_words:\n",
    "    if word not in stop_words:\n",
    "        filtered_transcript_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d267049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we can simply find word frequeny with NLTK's frequnecy distribution function\n",
    "from nltk import FreqDist\n",
    "\n",
    "transcript_fdist = FreqDist(filtered_transcript_words)\n",
    "transcript_fdist.most_common(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, as you can see, our list is topped by punctuation and contractions!\n",
    "\n",
    "# to remove punctuation, we can use Python's string library to create a list of punctuation\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "# and luckily, you can modify your stopwords and punctuation lists like any other list!\n",
    "# let's add \"n't\", \"'s\", and \"would\"\n",
    "# to add multiple elements to a list at once, we use extend() rather that append()\n",
    "stop_words.extend([\"n't\", \"'s\", 'would'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e041fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's re-run with our new stopwords and punctuation list to see the improved results\n",
    "filtered_transcript_words = []\n",
    "for word in transcript_words:\n",
    "    if word not in stop_words and word not in punctuation:\n",
    "        filtered_transcript_words.append(word)\n",
    "\n",
    "transcript_fdist = FreqDist(filtered_transcript_words)\n",
    "transcript_fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have a word frequency list, we can even use NLTK for concordance analysis (seeing word in context)\n",
    "# we can choose a word from the word frequency list, and search the original tokenized text for it after making it a Text object\n",
    "from nltk.text import Text\n",
    "\n",
    "text_list = Text(transcript_words)\n",
    "text_list.concordance(\"work\", lines=52)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72299453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is what our original Python script from the Python-I notebook now looks like with NLTK\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "transcript = open('Bette-Smith-Transcript.txt', encoding=\"utf-8\").read().lower()\n",
    "\n",
    "transcript_words = word_tokenize(transcript)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"n't\", \"'s\", 'would'])\n",
    "\n",
    "filtered_transcript_words = []\n",
    "for word in transcript_words:\n",
    "    if word not in stop_words and word not in punctuation:\n",
    "        filtered_transcript_words.append(word)\n",
    "\n",
    "transcript_fdist = FreqDist(filtered_transcript_words)\n",
    "transcript_fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f12e43",
   "metadata": {},
   "source": [
    "## Practice Activity #5: Investigate your own text 🔍\n",
    "For this activity, use a `.txt` file you have on hand, or download a plain text file from [Project Gutenburg](https://www.gutenberg.org/). Place it in the same folder as this notebook, then open it in your code and see if you can use the NLTK to perform a frequency distribution or concordance analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178de1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66ae0b5d",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/docs/) is a data analysis and manipulation tool, working with data in the form of a `dataframe`. A `dataframe` is a Python version of a spreadsheet!\n",
    "\n",
    "Like a spreadsheet, each column can be of a different type, and using Pandas means we can quickly perform a number of operations on our `dataframe` to prepare our data for use in analysis. To demonstrate functionality, we will be using an exported list of individuals accused of witchcraft in Scotland, from the [Survey of Scottish Witchcraft](https://www.shca.ed.ac.uk/Research/witches/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# we can add these arguments to set how many columns and rows we want Jupyter Notebook to display\n",
    "pd.options.display.max_columns = 70\n",
    "pd.options.display.max_rows = 70\n",
    "\n",
    "# we can import a CSV file very simply using Pandas's built in function\n",
    "witches_df = pd.read_csv(\"wdb_accused.csv\",  delimiter=\",\") \n",
    "witches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wow! that's a lot of confusing data!\n",
    "\n",
    "# to get the contents of only one column you can call the column by name\n",
    "print(witches_df['res_county'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can treat individual columns like lists by assigning them to a variable\n",
    "witches_residence = witches_df['res_county']\n",
    "print(type(witches_residence))\n",
    "\n",
    "# ...but this is still a pandas series, so to make a column into a list \"officially\" to avoid surprise errors, you can cast the column to be a list\n",
    "witches_residence = list(witches_df['res_county'])\n",
    "print(type(witches_residence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's a lot of columns, so let's reshape our dataframe to only have a few we're interested in\n",
    "witches_df = witches_df[['firstname', 'lastname', 'sex', 'age', 'res_county', 'maritalstatus', 'socioecstatus', 'occupation', 'notes']].copy()\n",
    "witches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# much better! now we can change a column name to make naming clearer\n",
    "witches_df = witches_df.rename(columns={\"res_county\": \"residing_county\"})\n",
    "witches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say we want to look at the occupation of each accused witch\n",
    "# there are a lot of NaN (Not a Number aka blank cells) which we can filter out using Pandas's .loc() and .notna() functions\n",
    "witches_df.loc[witches_df[\"occupation\"].notna()]\n",
    "\n",
    "# NOTE: there is also a function .isna() that does the opposite of .notna()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202db70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if I want to look only at those who were midwives, I can use .loc() with a comparison operator\n",
    "witches_df.loc[witches_df[\"occupation\"] == \"Midwife\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# like FreqDist in NLTK, Pandas has .value_counts() which will tally up the occurances of unique values in a given row\n",
    "# so let's check the distribution of occupations\n",
    "witches_df[\"occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want all basic statistics for numerical columns we can use .describe()\n",
    "# I'm interested to see the mean age of the accused\n",
    "witches_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to replace all instances of NaN in the dataframe with something more meaningful we can use the .fillna() function\n",
    "witches_df = witches_df.fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "witches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6287404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and take note that you can apply string methods to any column! \n",
    "# let's make everything in the \"notes\" column lowercase so it's normalised in case you need it for text analysis later\n",
    "witches_df[\"notes\"] = witches_df[\"notes\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "witches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, Pandas makes it really easy to export your dataframe as a CSV for publication or later use\n",
    "witches_df.to_csv(\"accused_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae429064",
   "metadata": {},
   "source": [
    "# Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write activity code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3efbd174",
   "metadata": {},
   "source": [
    "## Answer key (No peeking!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23eb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ea8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "litrev_df = pd.read_csv('digihum-lit-rev.csv', delimiter=\",\")\n",
    "\n",
    "litrev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e83ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "litrev_df = litrev_df[litrev_df[\"Description\"].notna()]\n",
    "\n",
    "abstracts_as_text = \"\"\n",
    "\n",
    "for i in litrev_df[\"Description\"]:\n",
    "    abstracts_as_text += i + \"\\n\"    \n",
    "    \n",
    "abstractTokens = word_tokenize(abstracts_as_text.lower())\n",
    "\n",
    "cleaned_abstractTokens = []\n",
    "\n",
    "for word in list(abstractTokens):\n",
    "    if word not in stopwords.words(\"english\") and word.isalpha():\n",
    "        cleaned_abstractTokens.append(word)\n",
    "\n",
    "abstracts_df = pd.DataFrame(cleaned_abstractTokens, columns =['uniqueWords'])\n",
    "        \n",
    "keywords = abstracts_df[\"uniqueWords\"].value_counts()\n",
    "\n",
    "keywords[100000000000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f6f7357",
   "metadata": {},
   "source": [
    "# Identifying and Solving Errors\n",
    "\n",
    "Try and correct the following errors! For more of a challenge, try and identify the errors before running the code 🔎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0599db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 1\n",
    "\n",
    "people = [\n",
    "    {'name': 'Jolene', 'birth_year': 1955, 'death_year': 1972},\n",
    "    {'name': 'George', 'birth_year': 1942, 'death_year': 2010},\n",
    "    {'name': 'Charlene', 'birth_year': 1927, 'death_year': 1941},\n",
    "    {'name': 'David', 'birth_year': 1830, 'death_year': 1923},\n",
    "    {'name': 'Eve', 'birth_year': 1899, 'death_year': 1940},\n",
    "]\n",
    "\n",
    "print(people[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a2f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 2\n",
    "\n",
    "# takes two arguments and returns their sum\n",
    "def add_numbers(x, y):\n",
    "    return x + y\n",
    "\n",
    "result = add_numbers(5, 10)\n",
    "\n",
    "print(\"The sum of the numbers is:\", result)\n",
    "print(\"The difference of the numbers is:\", result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef711cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 3\n",
    "\n",
    "year = 1955\n",
    "name = \"Jolene Barrie\"\n",
    "\n",
    "result = name + \" was born in \" + year\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd78af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 4\n",
    "\n",
    "people = [\n",
    "    {'name': 'Jolene', 'birth_year': 1955, 'death_year': 1972},\n",
    "    {'name': 'George', 'birth_year': 1942, 'death_year': 2010},\n",
    "    {'name': 'Charlene', 'birth_year': 1927, 'death_year': 1941},\n",
    "    {'name': 'David', 'birth_year': 1830, 'death_year' 1923},\n",
    "    {'name': 'Eve', 'birth_year': 1899, 'death_year': 1940},\n",
    "]\n",
    "\n",
    "for person in people:\n",
    "    print(\"Age at death: \" + str(person['death_year'] - person['birth_year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 5\n",
    "\n",
    "# convert strings to int\n",
    "def convert_to_num(year):\n",
    "    return int(year)\n",
    "\n",
    "year = \"1955\"\n",
    "name = \"Jolene Barrie\"\n",
    "\n",
    "convert_to_num(name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
