{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cadfbd47",
   "metadata": {},
   "source": [
    "# Loops\n",
    "\n",
    "Loops are used to repeat a process over and over until a given condition is met. It is similar to our process of, for example, searching for a specific quote in a text.\n",
    "\n",
    "Read sentence, \"Is this the sentence I'm looking for?\" you asking yourself, \"No\" your brain affirms-- and so you repeat this process of reading each sentence until you find the one you're looking for. Translating this process into computer, it would look something like this:\n",
    "\n",
    "```\n",
    "for every sentence on this page:\n",
    "    if this is the quote I am looking for:\n",
    "        I can stop reading, I've found it!\n",
    "    if this isn't the quote:\n",
    "        Let's read the next sentence.\n",
    "```\n",
    "\n",
    "In DH applications, for loops allow you to search a large amount of data very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d592078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a list of names\n",
    "list_of_names = [\"George Atherton\",\n",
    "                \"Marian Hyde\",\n",
    "                \"Sybil Dickenson\",\n",
    "                \"Sabina Dobson\",\n",
    "                \"Jessica Bradbury\",\n",
    "                \"Cindy Salter\",\n",
    "                \"Carolina McCabe\",\n",
    "                \"Glynis Graves\",\n",
    "                \"Laurie Dobson\",\n",
    "                \"Phoebe Watkins\",\n",
    "                \"Noel Boardman\"]\n",
    "\n",
    "# first, we can make a loop to \"iterate\" over the list with no conditions\n",
    "# it will simply continue to go over each item in the list until there is nothing left\n",
    "# to simply just print out every name in the list:\n",
    "\n",
    "for name in list_of_names:\n",
    "    print(name)\n",
    "\n",
    "# NOTE: \"name\" is a variable declared only in the loop, and it stores the item that the loop is presently looking at\n",
    "# in our case, in the first loop \"name\" = \"George Atherton\", and then after that name is printed, the loop repeats and \"name\" = \"Marian Hyde\", in the next loop \"name\" = \"Sybil Dickenson\", and so on until the end of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47484b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, like stated earlier, you'll more likely want to use a loop to find something relevant to your work\n",
    "# let's say we're only interested in people with the surname \"Dobson\"\n",
    "# we can use a combination of for loops and if statements to create a new list of only Dobsons!\n",
    "\n",
    "# declare your new list that we will add to\n",
    "only_dobsons = []\n",
    "\n",
    "for name in list_of_names:\n",
    "    # we check if this name includes \"Dobson\"\n",
    "    if \"Dobson\" in name:\n",
    "        # if this is True, we add this name to our new list\n",
    "        only_dobsons.append(name)\n",
    "\n",
    "print(only_dobsons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82c34a",
   "metadata": {},
   "source": [
    "# Functions and Libraries\n",
    "\n",
    "'Functions' are blocks of reuseable code; as you know by now, in Python there are many functions such as `print()` or `len()` which were designed to perform specific tasks when called. If in your own code you believe that there is a task you will need to repeat multiple times at various points, you can write a function yourself! For example, instead of having something like this: \n",
    "\n",
    "- Introduce NLTK through importing a text and then demonstrating individual functionalities\n",
    "- Same for Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12664f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find age of each person from records\n",
    "life_records = [[\"Cindy Salter\", \"Born: 1903\", \"Died: 1933\"], [\"Glynis Graves\", \"Born: 1911\", \"Died: 1989\"], [\"Noel Boardman\", \"Born: 1908\", \"Died: 1972\"]]\n",
    "\n",
    "cs_born = life_records[0][1]\n",
    "cs_death = life_records[0][2]\n",
    "\n",
    "# get only the number\n",
    "for word in cs_born.split():\n",
    "    if word.isdigit():\n",
    "        cs_born = int(word)\n",
    "\n",
    "for word in cs_death.split():\n",
    "    if word.isdigit():\n",
    "        cs_death = int(word)\n",
    "\n",
    "cs_age = cs_death - cs_born\n",
    "print(life_records[0][0] + \"'s age: \" + str(cs_age))\n",
    "\n",
    "gg_born = life_records[1][1]\n",
    "gg_death = life_records[1][2]\n",
    "\n",
    "# get only the number\n",
    "for word in gg_born.split():\n",
    "    if word.isdigit():\n",
    "        gg_born = int(word)\n",
    "\n",
    "for word in gg_death.split():\n",
    "    if word.isdigit():\n",
    "        gg_death = int(word)\n",
    "\n",
    "gg_age = gg_death - gg_born\n",
    "print(life_records[1][0] + \"'s age: \" + str(gg_age))\n",
    "\n",
    "nb_born = life_records[2][1]\n",
    "nb_death = life_records[2][2]\n",
    "\n",
    "# get only the number\n",
    "for word in nb_born.split():\n",
    "    if word.isdigit():\n",
    "        nb_born = int(word)\n",
    "\n",
    "for word in nb_death.split():\n",
    "    if word.isdigit():\n",
    "        nb_death = int(word)\n",
    "\n",
    "nb_age = nb_death - nb_born\n",
    "print(life_records[2][0] + \"'s age: \" + str(nb_age))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6d748",
   "metadata": {},
   "source": [
    "...We could have something much tidier and easier to read, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find age of each person from records\n",
    "life_records = [[\"Cindy Salter\", \"Born: 1903\", \"Died: 1933\"], [\"Glynis Graves\", \"Born: 1911\", \"Died: 1989\"], [\"Noel Boardman\", \"Born: 1908\", \"Died: 1972\"]]\n",
    "\n",
    "def find_age(record):\n",
    "    born = ''\n",
    "    death = ''\n",
    "    for word in record[1].split():\n",
    "        if word.isdigit():\n",
    "            born = int(word)\n",
    "\n",
    "    for word in record[2].split():\n",
    "        if word.isdigit():\n",
    "            death = int(word)\n",
    "\n",
    "    age = record[0] + \"'s age: \" + str(death - born)\n",
    "    return age\n",
    "\n",
    "print(find_age(life_records[0]))\n",
    "print(find_age(life_records[1]))\n",
    "print(find_age(life_records[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67a40d",
   "metadata": {},
   "source": [
    "Now, what can make your code even *tidier*, plus easier to read *and* write? Libraries! Also referred to as \"packages\", these helpful tools are essentially large collections of pre-written functions that you can install in your Python environment and import so that you can use these functions in your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45a0a0",
   "metadata": {},
   "source": [
    "## NLTK (Natural Language Tool Kit)\n",
    "\n",
    "In this workshop, we will be introducing two libraries which are necessities for any digital humanist's tool kit the first of which is [NLTK (Natural Language Tool Kit)](https://www.nltk.org/). This is an all-encompassing library to support work in natural language processing (NLP), a multidisciplinary field which deals with the interactions between \"natural\" human language and computers. It has its roots in linguisitics which is why it can do things like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# tagging PoS in inputted text\n",
    "text = word_tokenize(\"Be careful with that butter knife.\")\n",
    "nltk.pos_tag(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97b55f",
   "metadata": {},
   "source": [
    "...But as the `word_tokenize()` function hints at, NLTK is also excellent at preparing text for and performing textual analysis in a less particulated manner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ab267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# tokenization is the process of splitting strings into their individual \"tokens\"\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# to import a .txt file we use the \"open\" function, giving it the path to our text file and an instrution about what we want to do with the file\n",
    "# here, we would like to \"read\" our file into a variable so \n",
    "transcript = open('Bette-Smith-Transcript.txt').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could then tokenize by sentence, which splits the text into sentences\n",
    "transcript_sentences = sent_tokenize(transcript)\n",
    "transcript_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a14bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or more commonly, we can tokenize into words, which splits the sentences into its parts of speech\n",
    "transcript_words = word_tokenize(transcript)\n",
    "transcript_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now remember that huge block of stopwords manually typed out in the sample block of code from the first lesson? That comes built in to NLTK as you may have guessed from the earlier import statment\n",
    "# we can assign the NLTK stopwords to a variable like so:\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# and then remove the stopwords from out text using a loop to check if each word in the transcript and only keep the words that are NOT in out stopword list\n",
    "filtered_transcript_words = []\n",
    "for word in transcript_words:\n",
    "    if word not in stop_words:\n",
    "        filtered_transcript_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d267049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we can simply find word frequeny with NLTK's frequnecy distribution function\n",
    "from nltk import FreqDist\n",
    "\n",
    "transcript_fdist = FreqDist(filtered_transcript_words)\n",
    "transcript_fdist.most_common(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, as you can see, our list is topped by punctuation and contractions!\n",
    "\n",
    "# to remove punctuation, we can use Python's string library to create a list of punctuation\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation)\n",
    "\n",
    "# and luckily, you can modify your stopwords and punctuation lists like any other list!\n",
    "# let's add \"n't\", \"'s\", and \"would\"\n",
    "# to add multiple elements to a list at once, we use extend() rather that append()\n",
    "stop_words.extend([\"n't\", \"'s\", 'would'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e041fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's re-run with our new stopwords and punctuation list to see the improved results\n",
    "filtered_transcript_words = []\n",
    "for word in transcript_words:\n",
    "    if word not in stop_words and word not in punctuation:\n",
    "        filtered_transcript_words.append(word)\n",
    "\n",
    "transcript_fdist = FreqDist(filtered_transcript_words)\n",
    "transcript_fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afdfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have a word frequency list, we can even use NLTK for concordance analysis (seeing word in context)\n",
    "# we can choose a word from the word frequency list, and search the original tokenized text for it after making it a Text object\n",
    "from nltk.text import Text\n",
    "\n",
    "text_list = Text(transcript_words)\n",
    "text_list.concordance(\"work\", lines=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae0b5d",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/docs/) is a data analysis and manipulation tool, working with data in the form of a `dataframe`. A `dataframe` is a Python version of a spreadsheet!\n",
    "\n",
    "Like a spreadsheet, each column can be of a different type, and using Pandas means we can quickly perform a number of operations on our `dataframe` to prepare our data for use in analysis. To demonstrate functionality, we will be using an exported list of individuals accused of witchcraft in Scotland, from the [Survey of Scottish Witchcraft](https://www.shca.ed.ac.uk/Research/witches/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# we can add these arguments to set how many columns and rows we want Jupyter Notebook to display\n",
    "pd.options.display.max_columns = 70\n",
    "pd.options.display.max_rows = 70\n",
    "\n",
    "# we can import a CSV file very simply using Pandas's built in function\n",
    "witches_df = pd.read_csv(\"wdb_accused.csv\",  delimiter=\",\") \n",
    "witches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0a4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wow! that's a lot of confusing data!\n",
    "\n",
    "# to get the contents of only one column you can call the column by name\n",
    "witches_df['res_county']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can treat individual columns like lists by assigning them to a variable\n",
    "witches_residence = witches_df['res_county']\n",
    "type(witches_residence)\n",
    "\n",
    "# ...but this is still a pandas series, so to make a column into a list \"officially\" to avoid surprise errors, you can cast the column to be a list\n",
    "witches_residence = list(witches_df['res_county'])\n",
    "type(witches_residence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's a lot of columns, so let's reshape our dataframe to only have a few we're interested in\n",
    "witches_df = witches_df[['firstname', 'lastname', 'sex', 'age', 'res_county', 'maritalstatus', 'socioecstatus', 'occupation', 'notes']].copy()\n",
    "witches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# much better! now we can change a column name to make naming clearer\n",
    "witches_df = witches_df.rename(columns={\"res_county\": \"residing_county\"})\n",
    "witches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say we want to look at the occupation of each accused witch\n",
    "# there are a lot of NaN (Not a Number aka blank cells) which we can filter out using Pandas's .loc() and .notna() functions\n",
    "witches_df.loc[witches_df[\"occupation\"].notna()]\n",
    "\n",
    "# NOTE: there is also a function .isna() that does the opposite of .notna()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202db70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if I want to look only at those who were midwives, I can use .loc() with a comparison operator\n",
    "witches_df.loc[witches_df[\"occupation\"] == \"Midwife\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# like FreqDist in NLTK, Pandas has .value_counts() which will tally up the occurances of unique values in a given row\n",
    "# so let's check the distribution of occupations\n",
    "witches_df[\"occupation\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want all basic statistics for numerical columns we can use .describe()\n",
    "# I'm interested to see the mean age of the accused\n",
    "witches_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to replace all instances of NaN in the dataframe with something more meaningful we can use the .fillna() function\n",
    "witches_df = witches_df.fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6287404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and take note that you can apply string methods to any column! \n",
    "# let's make everything in the \"notes\" column lowercase so it's normalised in case you need it for text analysis later\n",
    "witches_df[\"notes\"] = witches_df[\"notes\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, Pandas makes it really easy to export your dataframe as a CSV for publication or later use\n",
    "witches_df.to_csv(\"accused_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae429064",
   "metadata": {},
   "source": [
    "# Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62fd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write activity code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbd174",
   "metadata": {},
   "source": [
    "## Answer key (No peeking!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23eb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ea8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "litrev_df = pd.read_csv('digihum-lit-rev.csv', delimiter=\",\")\n",
    "\n",
    "litrev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e83ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "litrev_df = litrev_df[litrev_df[\"Description\"].notna()]\n",
    "\n",
    "abstracts_as_text = \"\"\n",
    "\n",
    "for i in litrev_df[\"Description\"]:\n",
    "    abstracts_as_text += i + \"\\n\"\n",
    "    \n",
    "    \n",
    "abstractTokens = word_tokenize(abstracts_as_text.lower())\n",
    "\n",
    "cleaned_abstractTokens = []\n",
    "\n",
    "for word in list(abstractTokens):\n",
    "    if word not in stopwords.words(\"english\") and word.isalpha():\n",
    "        cleaned_abstractTokens.append(word)\n",
    "\n",
    "abstracts_df = pd.DataFrame(cleaned_abstractTokens, columns =['uniqueWords'])\n",
    "        \n",
    "keywords = abstracts_df[\"uniqueWords\"].value_counts()\n",
    "\n",
    "keywords[:25]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
