{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis on Filtered Dataframe (Mountain Pine Beetle Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in your CSV\n",
    "parksurvey_df = pd.read_csv(\"sksurveys.csv\",  delimiter=\",\")\n",
    "\n",
    "# replace \"nan\" floats with strings to prevent errors when filtering information\n",
    "parksurvey_df = parksurvey_df.fillna(\"Unknown\")\n",
    "\n",
    "parksurvey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of unique parks which we will use to filter dataframe\n",
    "list_of_parks = list(parksurvey_df[\"park_visited\"].unique())\n",
    "list_of_parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for park in list_of_parks:\n",
    "    # get rows where park_visited is the selected park\n",
    "    currpark_df = parksurvey_df.loc[parksurvey_df[\"park_visited\"] == park]\n",
    "    into_text = \"\"\n",
    "\n",
    "    # merge selected column into a text for textual analysis\n",
    "    for i in currpark_df[\"enjoys_most\"]:\n",
    "        print(i)\n",
    "        into_text += i + \"\\n\"\n",
    "    \n",
    "    print(into_text)\n",
    "\n",
    "# should you want to find who said what post analysis, use phrases outputted by concordance analysis to match back into larger dataset that includes demographics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to be a Good Webscraper (aka don't get blacklisted!)\n",
    "\n",
    "First and foremost, ensure that there is no clause on the website(s) you're planning to scrape that expilicitly state that it cannot be legally scraped and/or using the information provided by the website outside of it is a violation (this information can often be found in the Terms of Service). \n",
    "\n",
    "If that is clear, then essentially, to be an ethical webscraper you want your scraping process to imitate a human interacting with the website as much as possible, since that is how most websites are designed to be used! There are a few main ways this can be achieved:\n",
    "- **Wait times**: If you scrape a website too aggressively-- for example, asking a website for information or a new link very quickly one after another-- you risk overloading their servers (particularly an issue for older or smaller/more niche websites) which can cause the site to crash, or more likely, you to be blacklisted from accessing the website because you will be flagged as malicious/a bot. To combat this, we can add small wait times between our requests for information; these pauses give the servers a break!  \n",
    "- **Automate actions**: If possible, consider how you would interact with the webpage you want to scrape, and try imitating those steps through your code (discussed further in \"Selenium\").\n",
    "- **Use \"Inspect Element\" to analyse HTML**: Rather than request the entire HTML content of a webpage in your code to find the information you want to extract (hence, making more requests to their servers), look at the webpage's HTML through your browser's \"Inspect\" or \"Inspect Element\" tool!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a simple Python library that allows you to read the HTML content of a provided webpage into your code, allowing you to extract information present in it.\n",
    "Here are some examples of how the HTML of your page can be searched through and extracted from: https://beautiful-soup-4.readthedocs.io/en/latest/index.html#searching-the-tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping articles from news website\n",
    "\n",
    "# import two built in Python libraries:\n",
    "# requests allows you to request information from a website's server\n",
    "# sleep allows use to pause our code to give the servers a break\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# let's just get the first 15pgs of results from our search for now --> the following for loop imitates clicking \"next page\"\n",
    "article_links = []\n",
    "for i in range(1, 16):\n",
    "    # the URL we are requesting information from\n",
    "    url = \"https://betakit.com/query/fintech/page/\" + str(i)\n",
    "\n",
    "    # requesting the information --> \"user agent\" is telling the server to respond as if the request was coming from a Mozilla (Firefox) browser\n",
    "    req = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    # now we take a brief pause before continuing to manipulate the requested data\n",
    "    sleep(5)\n",
    "    html = req.text\n",
    "\n",
    "    # now we use BeautifulSoup to parse the extract HTML and create a \"soup\" object that can be used to search and find the exact information we want\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # to extract the link to each article, I found the HTML element using the \"Inspect\" tool and I am selecting all results which match that element using CSS\n",
    "    links = soup.select(\"h2.entry-title > a\")\n",
    "\n",
    "    # now, I am adding each link I extracted to a list that will be ongoing, capturing the article links from all 15pgs\n",
    "    for a in links:\n",
    "        article_links.append(a['href'])\n",
    "\n",
    "    print(\"On page \" + str(i) + \"! Sleeping to next page...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view list of article links --> there are 15 articles on each page so 15*15 means I should have 225 links\n",
    "article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can extract some metadata and content of each article and organize this information to a dataframe\n",
    "\n",
    "# create an empty dataframe with just the column headers\n",
    "articles_df = pd.DataFrame(columns=['author', 'date', 'title', 'content', 'tags'])\n",
    "\n",
    "for article in article_links:\n",
    "    # request information from the article webpage, just like we did before with the search page\n",
    "    url = article\n",
    "    req = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = req.text\n",
    "\n",
    "    sleep(10)\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "    # more information on the search + extract from HTML methods provided by BeautifulSoup in the link above\n",
    "    author = soup.find(\"a\", \"author url fn\").get_text()\n",
    "    date = soup.find(\"span\", \"entry-date\").get_text()\n",
    "    article_title = soup.find(\"h1\", \"entry-title\").get_text().strip()\n",
    "    article_content = soup.select(\"article > p\")[0].get_text().strip()\n",
    "    tags = soup.select(\"div#tags-box > a\")\n",
    "\n",
    "    # put info extracted from article into a list\n",
    "    new_article_row = [author, date, article_title, article_content, tags]\n",
    "    # add list to our dataframe as new row\n",
    "    articles_df.loc[len(articles_df)] = new_article_row  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view our completed dataframe!\n",
    "articles_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium\n",
    "\n",
    "[Selenium](https://selenium-python.readthedocs.io/) is a more advanced tool designed for automating web browsers through the use of **WebDrivers**; this essentially means the Selenium creates its own browser window from the browser you tell it to use, and in this window it performs tasks which you tell it to do through your code. This does mean that you shouldn't interact with the browser window Selenium creates, unless you want to risk messing up its scraping process! \n",
    "\n",
    "Selenium is often used in conjunction with BeautifulSoup, where Selenium performs an action that is necessary to make the information you need appear, and once that information has appeared you can use BeautifulSoup as usual.\n",
    "\n",
    "In the following example, the webpage required the user to search a location in a text box in order for the information on that location to appear, which necessitates the use of Selenium to automate scraping information from this site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example locations to search\n",
    "municipalities = ['Capixaba - AC',\n",
    "                'Cruzeiro Do Sul - AC',\n",
    "                'Cacimbinhas - AL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will select the Chrome WebDriver for Selenium to use in its window\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# create an empty dataframe with just the column headers\n",
    "descargas_df = pd.DataFrame(columns=['cidade/uf', 'densidade_descargas', 'rank_densidade_n', 'rank_densidade_e'])\n",
    "\n",
    "for m in municipalities:\n",
    "    # we want to open this webpage with our WebDriver in order interact with it \n",
    "    driver.get(\"http://www.inpe.br/webelat/homepage/#\")\n",
    "    \n",
    "    # Through the \"Inspect\" tool (in my own browser window, not the Selenium generated one!) I identify the HTML element which Selenium needs to interact with\n",
    "    # an input text field\n",
    "    inputElement = driver.find_element(\"id\", \"input_ranking\")\n",
    "    # the send_keys(m) command will enter the current selected municipality into the input text field identified in the previous line of code\n",
    "    inputElement.send_keys(m)\n",
    "    # this simulated you hitting the \"enter\" key and reveals the information we want\n",
    "    inputElement.send_keys(Keys.ENTER)\n",
    "\n",
    "    sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    \n",
    "    # now we can extract the information from the HTML element that was uncovered after searching for it using Selenium\n",
    "    scraped_tbl = soup.find(id=\"divRanking\").get_text()\n",
    "    # since it is extracted from HTML, this information is in an odd format so we'll put it into a list to make it more usable\n",
    "    scraped_tbl = scraped_tbl.split('\\n')\n",
    "    \n",
    "    # we just want the information following the \":\" in each extracted row of information\n",
    "    densidade_info = []\n",
    "    for i in scraped_tbl:\n",
    "        if len(i) > 1 and ':' in i:\n",
    "            i = i.strip().split(':')[1]\n",
    "            densidade_info.append(i)\n",
    "\n",
    "    print(densidade_info)\n",
    "\n",
    "    # now we can add the list of infomation as a new row in our dataframe\n",
    "    descargas_df.loc[len(descargas_df)] = densidade_info  \n",
    "    print('-------')\n",
    "\n",
    "# this closes the Selenium window\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view your completed dataframe!\n",
    "descargas_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
